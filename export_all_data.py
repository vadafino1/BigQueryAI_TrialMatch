#!/usr/bin/env python3
"""
Export ALL Real Data from BigQuery for Public Sharing
This exports complete datasets (not samples) while maintaining privacy
No PHI or patient identifiers are included
"""

import pandas as pd
import numpy as np
from google.cloud import bigquery
import json
import os
from datetime import datetime
import pyarrow.parquet as pq
import pyarrow as pa

# Configuration
PROJECT_ID = 'YOUR_PROJECT_ID'
DATASET_ID = 'clinical_trial_matching'
EXPORT_DIR = 'exported_data'

# Create export directory
os.makedirs(EXPORT_DIR, exist_ok=True)

print("="*80)
print("BIGQUERY 2025 - COMPLETE DATA EXPORT")
print("="*80)
print(f"Export started at: {datetime.now()}")
print(f"Output directory: {EXPORT_DIR}/")
print("="*80)

# Initialize BigQuery client
client = bigquery.Client(project=PROJECT_ID)

# 1. EXPORT ALL SEMANTIC MATCHES (200,000 rows)
print("\nüìä EXPORTING ALL MATCHES...")
matches_query = f"""
SELECT
  -- Create anonymous match IDs (no patient IDs!)
  CONCAT('MATCH_', LPAD(CAST(ROW_NUMBER() OVER (ORDER BY cosine_similarity DESC) AS STRING), 6, '0')) as match_id,
  cosine_similarity as similarity_score,
  match_quality,
  therapeutic_area,
  trial_phase as phase,
  trial_title as brief_title,
  -- Add explainability features
  CASE
    WHEN cosine_similarity >= 0.75 THEN 'High semantic alignment - strong conceptual match'
    WHEN cosine_similarity >= 0.65 THEN 'Moderate alignment - relevant therapeutic area'
    ELSE 'Exploratory match - potential cross-domain application'
  END as match_explanation,
  -- Performance metrics
  CURRENT_TIMESTAMP() as export_timestamp
FROM `{PROJECT_ID}.{DATASET_ID}.semantic_matches`
ORDER BY cosine_similarity DESC
"""

try:
    matches_df = client.query(matches_query).to_dataframe()
    matches_file = f"{EXPORT_DIR}/all_matches.csv"
    matches_df.to_csv(matches_file, index=False)
    print(f"‚úÖ Exported {len(matches_df):,} matches to {matches_file}")
    print(f"   File size: {os.path.getsize(matches_file) / 1024 / 1024:.1f} MB")
except Exception as e:
    print(f"‚ùå Error exporting matches: {e}")

# 2. EXPORT ALL PATIENT EMBEDDINGS (10,000 rows)
print("\nüß¨ EXPORTING ALL PATIENT EMBEDDINGS...")
patient_emb_query = f"""
SELECT
  -- Anonymous patient IDs
  CONCAT('PATIENT_EMB_', LPAD(CAST(ROW_NUMBER() OVER (ORDER BY patient_id) AS STRING), 5, '0')) as embedding_id,
  embedding,
  ARRAY_LENGTH(embedding) as embedding_dimension,
  trial_readiness,
  clinical_complexity,
  risk_category,
  -- Clustering information for explainability
  CASE
    WHEN clinical_complexity = 'HIGH_COMPLEXITY' THEN 'Complex Multi-System'
    WHEN clinical_complexity = 'MODERATE_COMPLEXITY' THEN 'Standard Care Profile'
    ELSE 'Routine Management'
  END as profile_category
FROM `{PROJECT_ID}.{DATASET_ID}.patient_embeddings`
WHERE embedding IS NOT NULL
"""

try:
    patient_emb_df = client.query(patient_emb_query).to_dataframe()

    # Save as Parquet for efficient storage of embeddings
    patient_emb_file = f"{EXPORT_DIR}/all_patient_embeddings.parquet"
    patient_emb_df.to_parquet(patient_emb_file, engine='pyarrow')
    print(f"‚úÖ Exported {len(patient_emb_df):,} patient embeddings to {patient_emb_file}")
    print(f"   File size: {os.path.getsize(patient_emb_file) / 1024 / 1024:.1f} MB")

    # Also create a metadata file
    patient_meta = {
        "total_embeddings": len(patient_emb_df),
        "embedding_dimension": 768,
        "readiness_distribution": patient_emb_df['trial_readiness'].value_counts().to_dict(),
        "complexity_distribution": patient_emb_df['clinical_complexity'].value_counts().to_dict()
    }

    with open(f"{EXPORT_DIR}/patient_embeddings_metadata.json", 'w') as f:
        json.dump(patient_meta, f, indent=2)

except Exception as e:
    print(f"‚ùå Error exporting patient embeddings: {e}")

# 3. EXPORT ALL TRIAL EMBEDDINGS (5,000 rows)
print("\nüíä EXPORTING ALL TRIAL EMBEDDINGS...")
trial_emb_query = f"""
SELECT
  nct_id,  -- Public trial IDs are safe to share
  embedding,
  ARRAY_LENGTH(embedding) as embedding_dimension,
  brief_title,
  therapeutic_area,
  phase,
  overall_status,
  enrollment_count,
  -- Additional context for understanding matches
  CASE
    WHEN therapeutic_area = 'ONCOLOGY' THEN 'Cancer and tumor-related conditions'
    WHEN therapeutic_area = 'CARDIAC' THEN 'Cardiovascular and heart diseases'
    WHEN therapeutic_area = 'DIABETES' THEN 'Metabolic and endocrine disorders'
    ELSE 'Various therapeutic applications'
  END as area_description
FROM `{PROJECT_ID}.{DATASET_ID}.trial_embeddings`
WHERE embedding IS NOT NULL
"""

try:
    trial_emb_df = client.query(trial_emb_query).to_dataframe()

    # Save as Parquet
    trial_emb_file = f"{EXPORT_DIR}/all_trial_embeddings.parquet"
    trial_emb_df.to_parquet(trial_emb_file, engine='pyarrow')
    print(f"‚úÖ Exported {len(trial_emb_df):,} trial embeddings to {trial_emb_file}")
    print(f"   File size: {os.path.getsize(trial_emb_file) / 1024 / 1024:.1f} MB")

    # Trial metadata
    trial_meta = {
        "total_trials": len(trial_emb_df),
        "embedding_dimension": 768,
        "therapeutic_distribution": trial_emb_df['therapeutic_area'].value_counts().to_dict(),
        "phase_distribution": trial_emb_df['phase'].value_counts().to_dict()
    }

    with open(f"{EXPORT_DIR}/trial_embeddings_metadata.json", 'w') as f:
        json.dump(trial_meta, f, indent=2)

except Exception as e:
    print(f"‚ùå Error exporting trial embeddings: {e}")

# 4. EXPORT AGGREGATED PERFORMANCE METRICS
print("\nüìà EXPORTING PERFORMANCE METRICS...")
metrics_query = f"""
WITH match_stats AS (
  SELECT
    COUNT(*) as total_matches,
    AVG(cosine_similarity) as avg_similarity,
    MAX(cosine_similarity) as max_similarity,
    MIN(cosine_similarity) as min_similarity,
    STDDEV(cosine_similarity) as stddev_similarity,
    COUNTIF(match_quality = 'GOOD_MATCH') as good_matches,
    COUNTIF(match_quality = 'FAIR_MATCH') as fair_matches,
    COUNTIF(match_quality = 'WEAK_MATCH') as weak_matches
  FROM `{PROJECT_ID}.{DATASET_ID}.semantic_matches`
)
SELECT * FROM match_stats
"""

try:
    metrics_df = client.query(metrics_query).to_dataframe()
    metrics_dict = metrics_df.to_dict('records')[0]

    # Add additional metrics
    metrics_dict.update({
        "export_date": datetime.now().isoformat(),
        "data_completeness": "100% - Full dataset exported",
        "privacy_status": "Fully anonymized - No PHI",
        "embedding_model": "text-embedding-004",
        "vector_dimension": 768,
        "index_type": "IVF (Inverted File Index)",
        "distance_metric": "COSINE"
    })

    metrics_file = f"{EXPORT_DIR}/performance_metrics.json"
    with open(metrics_file, 'w') as f:
        json.dump(metrics_dict, f, indent=2, default=str)
    print(f"‚úÖ Exported performance metrics to {metrics_file}")

except Exception as e:
    print(f"‚ùå Error exporting metrics: {e}")

# 5. EXPORT PERSONALIZED COMMUNICATIONS (Should be 100 now)
print("\nüìß EXPORTING PERSONALIZED COMMUNICATIONS...")
comms_query = f"""
SELECT
  CONCAT('COMM_', LPAD(CAST(ROW_NUMBER() OVER() AS STRING), 5, '0')) as communication_id,
  trial_id,
  trial_title,
  email_subject,
  email_body,
  coordinator_talking_points,
  sms_reminder,
  hybrid_score,
  match_confidence,
  generated_at
FROM `{PROJECT_ID}.{DATASET_ID}.personalized_communications`
ORDER BY hybrid_score DESC
"""

try:
    comms_df = client.query(comms_query).to_dataframe()
    comms_file = f"{EXPORT_DIR}/all_personalized_communications.json"
    comms_df.to_json(comms_file, orient='records', indent=2, default=str)
    print(f"‚úÖ Exported {len(comms_df):,} personalized communications to {comms_file}")
    print(f"   File size: {os.path.getsize(comms_file) / 1024 / 1024:.1f} MB")
except Exception as e:
    print(f"‚ùå Error exporting communications: {e}")

# 6. EXPORT CONSENT FORMS (Should be 50)
print("\nüìù EXPORTING CONSENT FORMS...")
consent_query = f"""
SELECT
  consent_id,
  trial_id,
  trial_title,
  therapeutic_area,
  consent_form_text,
  consent_summary,
  witness_statement,
  consent_version,
  consent_status,
  generated_at
FROM `{PROJECT_ID}.{DATASET_ID}.consent_forms_generated`
ORDER BY generated_at DESC
"""

try:
    consent_df = client.query(consent_query).to_dataframe()
    consent_file = f"{EXPORT_DIR}/all_consent_forms.json"
    consent_df.to_json(consent_file, orient='records', indent=2, default=str)
    print(f"‚úÖ Exported {len(consent_df):,} consent forms to {consent_file}")
    print(f"   File size: {os.path.getsize(consent_file) / 1024 / 1024:.1f} MB")
except Exception as e:
    print(f"‚ùå Error exporting consent forms: {e}")

# 7. CREATE DATA DICTIONARY
print("\nüìö CREATING DATA DICTIONARY...")
data_dictionary = {
    "all_matches.csv": {
        "description": "Complete semantic matching results between patients and trials",
        "rows": 200000,
        "columns": {
            "match_id": "Anonymous match identifier",
            "similarity_score": "Cosine similarity between patient and trial embeddings (0-1)",
            "match_quality": "Categorical quality assessment (GOOD/FAIR/WEAK)",
            "therapeutic_area": "Primary therapeutic focus of the trial",
            "phase": "Clinical trial phase",
            "brief_title": "Public trial title",
            "match_explanation": "Human-readable explanation of match quality"
        }
    },
    "all_patient_embeddings.parquet": {
        "description": "Complete set of patient profile embeddings",
        "rows": 10000,
        "columns": {
            "embedding_id": "Anonymous patient embedding identifier",
            "embedding": "768-dimensional vector representation",
            "trial_readiness": "Patient's readiness for trial enrollment",
            "clinical_complexity": "Overall clinical complexity score",
            "risk_category": "Risk stratification category",
            "profile_category": "Human-readable profile description"
        }
    },
    "all_trial_embeddings.parquet": {
        "description": "Complete set of clinical trial embeddings",
        "rows": 5000,
        "columns": {
            "nct_id": "Official ClinicalTrials.gov identifier",
            "embedding": "768-dimensional vector representation",
            "brief_title": "Official trial title",
            "therapeutic_area": "Primary therapeutic focus",
            "phase": "Clinical trial phase",
            "sponsor_type": "Type of trial sponsor",
            "area_description": "Human-readable therapeutic area description"
        }
    }
}

dict_file = f"{EXPORT_DIR}/data_dictionary.json"
with open(dict_file, 'w') as f:
    json.dump(data_dictionary, f, indent=2)
print(f"‚úÖ Created data dictionary at {dict_file}")

# 8. SUMMARY REPORT
print("\n" + "="*80)
print("EXPORT SUMMARY")
print("="*80)

total_size = sum(
    os.path.getsize(os.path.join(EXPORT_DIR, f))
    for f in os.listdir(EXPORT_DIR)
    if os.path.isfile(os.path.join(EXPORT_DIR, f))
) / 1024 / 1024

print(f"‚úÖ Total files exported: {len(os.listdir(EXPORT_DIR))}")
print(f"‚úÖ Total size: {total_size:.1f} MB")
print(f"‚úÖ Privacy status: Fully anonymized - No PHI or patient identifiers")
print(f"‚úÖ Ready for public sharing via Google Drive")
print("\nüìÅ Files created in: {}/".format(os.path.abspath(EXPORT_DIR)))
print("\nNext steps:")
print("1. Upload to Google Drive public folder")
print("2. Update notebook with Google Drive URLs")
print("3. Test download and loading in demo notebook")
print("="*80)